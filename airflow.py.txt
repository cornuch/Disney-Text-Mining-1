from airflow import DAG
from airflow.operators.bash_operator import BashOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'me',
    'start_date': datetime(2022, 1, 1),
    'depends_on_past': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'my_dag_id',
    default_args=default_args,
    schedule_interval=timedelta(hours=1),
)

task = BashOperator(
    task_id='my_task_id',
    bash_command='echo "Hello World!"',
    dag=dag,
)



from airflow.operators.python_operator import PythonOperator

def my_function():
    print("Hello World!")

task = PythonOperator(
    task_id='my_task_id',
    python_callable=my_function,
    dag=dag,
)


En Airflow, la fonction PythonOperator.provide_context permet de passer des informations supplémentaires à la fonction Python appelée par l'opérateur.

La fonction provide_context prend en argument un booléen qui indique si la fonction appelée doit recevoir le contexte de la tâche en cours d'exécution. Le contexte comprend des informations telles que les variables d'environnement, les paramètres de la DAG, les paramètres de la tâche, etc.

Voici un exemple d'utilisation de provide_context pour passer des variables d'environnement à la fonction appelée:




Dans Airflow, l'argument pool de la fonction PythonOperator permet de spécifier dans quel "pool" de worker les tâches de cet opérateur seront exécutées.

Un pool est un groupe de worker qui peuvent être utilisés pour exécuter des tâches. Il permet de limiter le nombre de tâches en cours d'exécution simultanées pour un groupe donné de tâches.

Voici un exemple d'utilisation de l'argument pool pour assigner la tâche à un pool spécifique:

task = PythonOperator(
    task_id='my_task_id',
    python_callable=my_function,
    dag=dag,
    pool='my_pool'
)
Il est important de noter que vous devez configurer les pools dans l'environnement Airflow avant de pouvoir les utiliser dans vos tâches. Cela peut se faire dans la configuration d'Airflow, dans la section [celery] et pool .

Il est aussi important de noter que si vous n'avez pas configuré de pools, les tâches seront exécutées dans le pool par défaut 'default'.




Pour mettre en place un ETL (Extract-Transform-Load) avec Airflow, vous devrez créer au moins les fichiers Python suivants:

    DAG Python file : Ce fichier contiendra la définition de votre DAG (Directed Acyclic Graph) Airflow, qui décrit les étapes de votre pipeline ETL et les dépendances entre ces étapes. Il contiendra également les paramètres de base pour votre DAG, tels que les propriétaires, la fréquence de planification, etc.

    Operators Python files : Ces fichiers contiendront les définitions des opérateurs Airflow que vous utiliserez pour exécuter les étapes de votre pipeline ETL. Il peut s'agir d'opérateurs Python, Bash, SQL, HTTP, etc., selon les besoins de votre pipeline. Ces opérateurs peuvent être des tâches simples ou des tâches plus complexe qui utilisent des librairies spécifiques.

    Helper Python files : Ces fichiers contiendront des fonctions ou des classes utilitaires qui peuvent être utilisées par les opérateurs dans les étapes de votre pipeline ETL. Par exemple, vous pourriez avoir des fonctions pour valider les données, transformer les données ou gérer les erreurs.

Il est important de noter que ces fichiers doivent être stockés dans un répertoire dédié à Airflow pour qu'il puisse les détecter et les utiliser pour planifier les tâches.

Il est également important de noter que pour mettre en place un pipeline ETL, il est fortement recommandé de respecter les bonnes pratiques de développement de software, comme utiliser des tests unitaires pour valider le bon fonctionnement de chaque étape, utiliser un outil de versionning pour suivre les évolutions, utiliser des mécanismes de logging pour suivre les erreurs,etc.